{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "overalltraining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wtgci6erLyL"
      },
      "source": [
        "# Importing required libraries \n",
        "# Keras\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, Model, model_from_json\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
        "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# sklearn\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Other  \n",
        "import librosa\n",
        "import librosa.display\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import glob \n",
        "import os\n",
        "import pickle\n",
        "import IPython.display as ipd  # To play sound in the notebook\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOzxqiZWsO3I"
      },
      "source": [
        "# lets pick up the meta-data that we have which corresponds to the datasets \n",
        "!pwd\n",
        "ref = pd.read_csv(\"/content/dataset/Data_path.csv\")\n",
        "ref.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0TiBmfAtpjJ"
      },
      "source": [
        "# Note this takes a long time depending on the dataset size and the GPU specs of the runtime\n",
        "df = pd.DataFrame(columns=['feature'])\n",
        "\n",
        "# loop feature extraction over the entire dataset\n",
        "counter=0\n",
        "for index,path in enumerate(ref.path):\n",
        "    X, sample_rate = librosa.load(path\n",
        "                                  , res_type='kaiser_fast'\n",
        "                                  ,duration=2.5\n",
        "                                  ,sr=44100\n",
        "                                  ,offset=0.5\n",
        "                                 )\n",
        "    sample_rate = np.array(sample_rate)\n",
        "    \n",
        "    # mean as the feature. Could do min and max etc as well. \n",
        "    mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
        "                                        sr=sample_rate, \n",
        "                                        n_mfcc=13),\n",
        "                    axis=0)\n",
        "    df.loc[counter] = [mfccs]\n",
        "    counter=counter+1   \n",
        "\n",
        "# Check a few records to make sure its processed successfully\n",
        "print(len(df))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaipSVMqt9s_"
      },
      "source": [
        "# Now extract the mean bands to its own feature columns\n",
        "df = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\n",
        "df[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-gKNj2AuIFH"
      },
      "source": [
        "# replace NA with 0\n",
        "df=df.fillna(0)\n",
        "print(df.shape)\n",
        "df[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWfit87IuLt4"
      },
      "source": [
        "# Split between train and test \n",
        "X_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n",
        "                                                    , df.labels\n",
        "                                                    , test_size=0.25\n",
        "                                                    , shuffle=True\n",
        "                                                    , random_state=42\n",
        "                                                   )\n",
        "\n",
        "# Lets see how the data present itself before normalisation \n",
        "X_train[150:160]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPxA_18cuZrH"
      },
      "source": [
        "# Lts do data normalization \n",
        "mean = np.mean(X_train, axis=0)\n",
        "std = np.std(X_train, axis=0)\n",
        "\n",
        "X_train = (X_train - mean)/std\n",
        "X_test = (X_test - mean)/std\n",
        "\n",
        "# Check the dataset now \n",
        "X_train[150:160]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzXxgtezufz3"
      },
      "source": [
        "# Lets few preparation steps to get it into the correct format for Keras \n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# one hot encode the target \n",
        "lb = LabelEncoder()\n",
        "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
        "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n",
        "\n",
        "print(X_train.shape)\n",
        "print(lb.classes_)\n",
        "#print(y_train[0:10])\n",
        "#print(y_test[0:10])\n",
        "\n",
        "# Pickel the lb object for future use \n",
        "filename = 'labels'\n",
        "outfile = open(filename,'wb')\n",
        "pickle.dump(lb,outfile)\n",
        "outfile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WqsveLruzho"
      },
      "source": [
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_1HqRG3u_KA"
      },
      "source": [
        "# New model -- this is the model architecture. make this simple or complex depending on resource availability\n",
        "model = Sequential()\n",
        "model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(256, 8, padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(128, 8, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 8, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 8, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(128, 8, padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(64, 8, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv1D(64, 8, padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(14)) # Target class number\n",
        "model.add(Activation('softmax'))\n",
        "# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
        "# opt = keras.optimizers.Adam(lr=0.0001)\n",
        "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NOgWg50vO3o"
      },
      "source": [
        "## now train the model - this may take a long time depending on resources, dataset and architecture\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
        "model_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE4TyNHivULq"
      },
      "source": [
        "## show training progress\n",
        "plt.plot(model_history.history['loss'])\n",
        "plt.plot(model_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dNSry0GvwnQ"
      },
      "source": [
        "# Save model and weights\n",
        "model_name = 'Emotion_ModelvXX.h5'\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Save model and weights at %s ' % model_path)\n",
        "\n",
        "# Save the model to disk\n",
        "model_json = model.to_json()\n",
        "with open(\"model_jsonvXX.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TW_GJqEwLxY"
      },
      "source": [
        "# loading json and model architecture \n",
        "json_file = open('model_jsonvXX.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# load weights into new model\n",
        "loaded_model.load_weights(\"saved_models/Emotion_ModelvXX.h5\")\n",
        "print(\"Loaded model from disk\")\n",
        " \n",
        "# Keras optimiser\n",
        "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X1h5gkWwXMw"
      },
      "source": [
        "preds = loaded_model.predict(X_test, \n",
        "                         batch_size=16, \n",
        "                         verbose=1)\n",
        "\n",
        "preds=preds.argmax(axis=1)\n",
        "# predictions \n",
        "preds = preds.astype(int).flatten()\n",
        "preds = (lb.inverse_transform((preds)))\n",
        "preds = pd.DataFrame({'predictedvalues': preds})\n",
        "\n",
        "# Actual labels\n",
        "actual=y_test.argmax(axis=1)\n",
        "actual = actual.astype(int).flatten()\n",
        "actual = (lb.inverse_transform((actual)))\n",
        "actual = pd.DataFrame({'actualvalues': actual})\n",
        "\n",
        "# Lets combined both of them into a single dataframe\n",
        "finaldf = actual.join(preds)\n",
        "finaldf[170:180]\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}